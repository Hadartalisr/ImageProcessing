{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.special import comb\n",
    "import math\n",
    "import concurrent.futures\n",
    "from copy import deepcopy\n",
    "import pywt\n",
    "\n",
    "def open_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_gray_img_mat(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "\n",
    "def expand_image(img, expanded_size, kernel_size, ratio=2):\n",
    "    # Create the expanded image with zeros\n",
    "    expanded_img = np.zeros(expanded_size)\n",
    "    # Create the gaussian kernel\n",
    "    (kernel, weights) = get_gaussian_kernel(kernel_size)\n",
    "    kernel = kernel / (weights / 4)\n",
    "    # Upsample the input image\n",
    "    # Adjust the slicing to match the desired size exactly\n",
    "    expanded_img[:img.shape[0] * ratio:ratio, :img.shape[1] * ratio:ratio] = img\n",
    "    # Perform the blur\n",
    "    expanded_img = np.round(convolve(expanded_img, kernel))\n",
    "    return expanded_img\n",
    "\n",
    "\n",
    "def convolve(image, kernel):\n",
    "    # Perform the convolution using scipy.signal.convolve2d\n",
    "    result = convolve2d(image, kernel, mode='same', boundary='symm')\n",
    "    return result\n",
    "\n",
    "\n",
    "# create a function that returns the gaussian kernel for a given size and the total sum of the kernel\n",
    "# for example ofr size=5 return kernel = np.array([[1, 4, 6, 4, 1],\n",
    "# [4, 16, 24, 16, 4], [6, 24, 36, 24, 6], [4, 16, 24, 16, 4], [1, 4, 6, 4, 1]]) and weight = 256\n",
    "def get_gaussian_kernel(size):\n",
    "    # Calculate the kernel\n",
    "    n = math.floor(size)\n",
    "    kernel = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            kernel[i, j] = comb(n - 1, i) * comb(n - 1, j)\n",
    "\n",
    "    # Normalize the kernel to make the sum an integer\n",
    "    kernel_sum = np.sum(kernel)\n",
    "    kernel_factor = int(np.ceil(kernel_sum))\n",
    "    kernel = (kernel * kernel_factor / kernel_sum).astype(int)\n",
    "\n",
    "    return kernel, kernel_factor\n",
    "\n",
    "\n",
    "def reduce_image(img, reduced_img_size, kernel_size=5, should_blur=True):\n",
    "    # Create the gaussian kernel\n",
    "    (kernel, weights) = get_gaussian_kernel(kernel_size)\n",
    "    kernel = kernel / weights\n",
    "    # Perform the blur\n",
    "    if should_blur:\n",
    "        img = np.round(convolve(img, kernel))\n",
    "\n",
    "    # Use array slicing and reshaping to select every second element\n",
    "    reduced_img = img[:reduced_img_size[0] * 2:2, :reduced_img_size[1] * 2:2]\n",
    "\n",
    "    return reduced_img\n",
    "\n",
    "\n",
    "def get_reduced_size(img, factor=2):\n",
    "    return img.shape[0] // factor, img.shape[1] // factor\n",
    "\n",
    "\n",
    "def get_max_level(img):\n",
    "    return int(np.log2(min(img.shape[0], img.shape[1]))) - 1\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def create_pyramid(img, kernel_size, scale = 10):\n",
    "    pyramid = []\n",
    "    pyramid.append({'G': img})\n",
    "    reduced_img = img\n",
    "    level = min(get_max_level(img), scale+1)\n",
    "    for i in range(1, get_max_level(img)):\n",
    "        reduced_img_size = get_reduced_size(reduced_img, 2)\n",
    "        reduced_img = reduce_image(reduced_img, reduced_img_size, kernel_size)\n",
    "        expanded_image_size = pyramid[i - 1]['G'].shape\n",
    "        expanded_img = expand_image(reduced_img, expanded_image_size, kernel_size)\n",
    "        pyramid.append({'G': reduced_img, 'Expanded_G': expanded_img})\n",
    "\n",
    "    for i in range(len(pyramid) - 1):\n",
    "        pyramid[i]['L'] = pyramid[i]['G'] - pyramid[i + 1]['Expanded_G']\n",
    "    pyramid[-1]['L'] = pyramid[-1]['G']\n",
    "    return pyramid\n",
    "\n",
    "def get_mask_pyramid(mask):\n",
    "    mask = np.ceil(mask / 255)\n",
    "    mask_pyramid = []\n",
    "    mask_pyramid.append({'G': mask})\n",
    "    reduced_mask = mask\n",
    "    for i in range(1, get_max_level(mask)):\n",
    "        reduced_mask_size = get_reduced_size(reduced_mask, 2)\n",
    "        reduced_mask = reduce_image(reduced_mask, reduced_mask_size)\n",
    "        mask_pyramid.append({'G': reduced_mask})\n",
    "    return mask_pyramid\n",
    "\n",
    "\n",
    "def get_images_for_feature_matching(img_high_res_gray, img_low_res_gray, scale):\n",
    "    high_res_pyramid = create_pyramid(img_high_res_gray, gaussian_kernel_size, scale)\n",
    "    low_res_pyramid = create_pyramid(img_low_res_gray, gaussian_kernel_size, scale)\n",
    "    \n",
    "    high_res_img = cv2.normalize(high_res_pyramid[scale]['G'], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    low_res_img = cv2.normalize(low_res_pyramid[scale]['G'], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    return high_res_img, low_res_img\n",
    "\n",
    "\n",
    "def get_original_coordinates(points, scale):\n",
    "    return points * (2 ** scale)\n",
    "\n",
    "\n",
    "def get_colored_img_high_res_wrapped(colored_img_high_res, H):\n",
    "    h, w = colored_img_high_res.shape[:2]\n",
    "    colored_img_warped = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for i in range(3):\n",
    "        colored_img_warped[:, :, i] = cv2.warpPerspective(colored_img_high_res[:, :, i], H, (w, h))\n",
    "    return colored_img_warped\n",
    "\n",
    "\n",
    "\n",
    "def blend_pyramids(pyramid1, pyramid2, mask_pyramid, max_level=100):\n",
    "    blended_pyramid = []\n",
    "    i = 0\n",
    "    max_level = min(max_level, len(pyramid1))\n",
    "    for i in range(max_level - 1):\n",
    "        blended_pyramid.append(\n",
    "            {'L': pyramid1[i]['L'] * mask_pyramid[i]['G'] + (1 - mask_pyramid[i]['G']) * pyramid2[i]['L']})\n",
    "    # the last level is blended differently\n",
    "    blended_pyramid.append(\n",
    "        {'L': pyramid1[i + 1]['G'] * mask_pyramid[i + 1]['G'] + (1 - mask_pyramid[i + 1]['G']) * pyramid2[i + 1]['G']})\n",
    "    return blended_pyramid\n",
    "\n",
    "def reconstruct_image(pyramid, kernel_size):\n",
    "    reconstructed_img = pyramid[-1]['L']\n",
    "    for i in range(len(pyramid) - 2, -1, -1):\n",
    "        expanded_img = expand_image(reconstructed_img, pyramid[i]['L'].shape, kernel_size)\n",
    "        reconstructed_img = expanded_img + pyramid[i]['L']\n",
    "    return reconstructed_img\n",
    "\n",
    "\n",
    "def blend_images(img1, img2, mask, max_level, kernel_size):\n",
    "    with (concurrent.futures.ThreadPoolExecutor()):\n",
    "        img1_pyramid_a, img1_pyramid_b, img1_pyramid_c = create_pyramid(img1[:, :, 0], kernel_size), create_pyramid(\n",
    "            img1[:, :, 1], kernel_size), create_pyramid(img1[:, :, 2], kernel_size)\n",
    "        img2_pyramid_a, img2_pyramid_b, img2_pyramid_c = create_pyramid(img2[:, :, 0], kernel_size), create_pyramid(\n",
    "            img2[:, :, 1], kernel_size), create_pyramid(img2[:, :, 2], kernel_size)\n",
    "        mask_pyramid = get_mask_pyramid(mask)\n",
    "        blended_pyramid_a = blend_pyramids(img1_pyramid_a, img2_pyramid_a, mask_pyramid, max_level)\n",
    "        blended_pyramid_b = blend_pyramids(img1_pyramid_b, img2_pyramid_b, mask_pyramid, max_level)\n",
    "        blended_pyramid_c = blend_pyramids(img1_pyramid_c, img2_pyramid_c, mask_pyramid, max_level)\n",
    "    reconstructed_img_a = reconstruct_image(blended_pyramid_a, kernel_size)\n",
    "    reconstructed_img_b = reconstruct_image(blended_pyramid_b, kernel_size)\n",
    "    reconstructed_img_c = reconstruct_image(blended_pyramid_c, kernel_size)\n",
    "    \n",
    "    #normalize the image\n",
    "    reconstructed_img_a = cv2.normalize(reconstructed_img_a, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    reconstructed_img_b = cv2.normalize(reconstructed_img_b, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    reconstructed_img_c = cv2.normalize(reconstructed_img_c, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "    return np.dstack((reconstructed_img_a, reconstructed_img_b, reconstructed_img_c))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "scale_for_feature_matching = 2\n",
    "gaussian_kernel_size = 5\n",
    "ratio_1nn_to_2nn = 0.8\n",
    "max_error_for_homography = 5\n",
    "\n",
    "\n",
    "def ex4(img_high_res_path, img_low_res_path):\n",
    "    # open the images\n",
    "    colored_img_high_res = open_image(img_high_res_path)\n",
    "    colored_img_low_res = open_image(img_low_res_path)\n",
    "    # convert the images to gray scale\n",
    "    img_high_res_gray = get_gray_img_mat(colored_img_high_res)\n",
    "    img_low_res_gray = get_gray_img_mat(colored_img_low_res)       \n",
    "    # create the pyramids (gaussian and laplacian) for the images\n",
    "\n",
    "    \n",
    "    # get the images for feature matching\n",
    "    img_high_res_gray, img_low_res_gray = get_images_for_feature_matching(img_high_res_gray, img_low_res_gray, scale_for_feature_matching)\n",
    "    \n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp_high = sift.detect(img_high_res_gray, None)\n",
    "    kp_low= sift.detect(img_low_res_gray, None)\n",
    "    \n",
    "    # compute the descriptors\n",
    "    kp_high, des_high = sift.compute(img_high_res_gray, kp_high)\n",
    "    kp_low, des_low = sift.compute(img_low_res_gray, kp_low)\n",
    "    \n",
    "    # BFMatcher with default params\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des_high, des_low, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_1nn_to_2nn * n.distance:\n",
    "            good.append(m)\n",
    "     \n",
    "     \n",
    "    # Extract keypoints from good matches\n",
    "    kp_high_matched = [kp_high[m.queryIdx] for m in good]\n",
    "    kp_low_matched = [kp_low[m.trainIdx] for m in good]\n",
    "    \n",
    "    # covnert keypoints to numpy arrays\n",
    "    src_pts = np.float32([kp.pt for kp in kp_high_matched]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp.pt for kp in kp_low_matched]).reshape(-1, 1, 2)\n",
    "    \n",
    "    src_pts = get_original_coordinates(src_pts, scale_for_feature_matching)\n",
    "    dst_pts = get_original_coordinates(dst_pts, scale_for_feature_matching)\n",
    "    \n",
    "    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, max_error_for_homography)\n",
    "    colored_high_img_wraped = get_colored_img_high_res_wrapped(colored_img_high_res, H)\n",
    "    mask = (colored_high_img_wraped != 0).astype(np.uint8)[:,:, 0]\n",
    "    \n",
    "    blended_image = blend_images(colored_high_img_wraped, colored_img_low_res, mask, 2, gaussian_kernel_size)\n",
    "    return blended_image\n",
    "\n",
    "# usage example\n",
    "\n",
    "# img_low_res_path = './assets/lake_low_res.jpg'\n",
    "# img_high_res_path = './assets/lake_high_res.png'\n",
    "\n",
    "# img_high_res_path = './assets/desert_high_res.png'\n",
    "# img_low_res_path = './assets/desert_low_res.jpg'\n",
    "# \n",
    "# blended_image = ex4(img_high_res_path, img_low_res_path)\n",
    "# plt.imshow(cv2.cvtColor(blended_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "# manually implementation of the core functionality\n",
    "\n",
    "# threshold = 0.3\n",
    "# harris_window_size = 5\n",
    "# k = 0.04\n",
    "# suppression_window_size = 5\n",
    "# scales = 3\n",
    "# descriptor_window_size = 8\n",
    "# scale_added_to_get_descriptor = 2\n",
    "# matches_ratio_threshold = 0.8\n",
    "\n",
    "# implementing Harris corner detection\n",
    "def harris_corner_detection(gray_img, harris_window_size, k):\n",
    "    img_gaussian = cv2.GaussianBlur(gray_img,(3,3),0)\n",
    "\n",
    "    Ix = cv2.Sobel(img_gaussian, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    Iy = cv2.Sobel(img_gaussian, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    Ix2=np.square(Ix)\n",
    "    Iy2=np.square(Iy)\n",
    "    Ixy=Ix*Iy\n",
    "\n",
    "    harris_window = np.ones((harris_window_size, harris_window_size))\n",
    "    Sx2 = convolve2d(Ix2, harris_window, mode='same')\n",
    "    Sy2 = convolve2d(Iy2, harris_window, mode='same')\n",
    "    Sxy = convolve2d(Ixy, harris_window, mode='same')\n",
    "\n",
    "    det = Sx2 * Sy2 - (Sxy ** 2)\n",
    "    trace = Sx2 + Sy2\n",
    "\n",
    "    R = det - k * (trace ** 2)\n",
    "    cv2.normalize(R, R, 0, 1, cv2.NORM_MINMAX)    \n",
    "    return R\n",
    "\n",
    "# Suppress non-maximum responses\n",
    "def non_maximum_suppression(corners, suppression_window_size):\n",
    "    # Apply maximum filter for non-maximum suppression\n",
    "    suppressed_corners = corners * (corners == maximum_filter(corners, footprint=np.ones((suppression_window_size, suppression_window_size))))\n",
    "    return suppressed_corners\n",
    "\n",
    "\n",
    "# create multiscale Harris corner detector\n",
    "def multiscale_harris_corner_detector(pyramid, scales, harris_window_size, k, suppression_window_size):\n",
    "    # Initialize corners tensor\n",
    "    corners_arr = np.zeros((scales,), dtype=np.ndarray)\n",
    "    # Iterate over the scales in the pyramid \n",
    "    for scale in range(scales):\n",
    "        # Perform the Harris corner detection\n",
    "        corners = harris_corner_detection(pyramid[scale]['G'], harris_window_size, k)     \n",
    "        # Add the corners to the corners tensor\n",
    "        if suppression_window_size:\n",
    "            corners = non_maximum_suppression(corners, suppression_window_size)\n",
    "        corners_arr[scale] = corners\n",
    "    return corners_arr\n",
    "\n",
    "\n",
    "# create a function the iterates over the matches for each descriptor adds its real coordinates in the 0 scale \n",
    "# and the coordinates of the match in the 0 scale\n",
    "def get_matches_with_original_coordinates(matches):\n",
    "    matches_with_original_coordinates = []\n",
    "    for match in matches:\n",
    "        x, y, scale, theta, descriptor = match[0]\n",
    "        x_original, y_original = get_original_coordinates(match[0])\n",
    "        x_prime, y_prime, scale_prime, theta_prime, descriptor_prime = match[1]\n",
    "        x_prime_original, y_prime_original = get_original_coordinates(match[1])\n",
    "        matches_with_original_coordinates.append([(x_original, y_original, scale, theta, descriptor), (x_prime_original, y_prime_original, scale_prime, theta_prime, descriptor_prime)])        \n",
    "    return matches_with_original_coordinates\n",
    "\n",
    "# implement RANSAC algorithm - each iteration, randomly select 4 matches and calculate the homography matrix\n",
    "# then, calculate the number of inliers for each match and select the best homography matrix\n",
    "# finally, return the best homography matrix\n",
    "def ransac(matches_with_original_coordinates, iterations, threshold):\n",
    "    best_homography = None\n",
    "    best_inliers = 0\n",
    "    best_inliers_matches = []\n",
    "    for iter in range(iterations):\n",
    "        # Randomly select 4 indexes from the matches\n",
    "        random_indexes = np.random.choice(len(matches_with_original_coordinates), 4, replace=False)\n",
    "        random_matches = [matches_with_original_coordinates[i] for i in random_indexes]\n",
    "        # Calculate the homography matrix\n",
    "        H = calculate_homography(random_matches)\n",
    "        # Calculate the number of inliers\n",
    "        inliers = 0\n",
    "        inliers_matches = []\n",
    "        \n",
    "        for match in matches_with_original_coordinates:\n",
    "            # Apply the transformation to the point\n",
    "            p = np.array([match[0][0], match[0][1], 1])\n",
    "            p_prime = np.dot(H, p)\n",
    "            p_prime = (p_prime / p_prime[2])[:2]\n",
    "            # Calculate the distance between the transformed point and the second point\n",
    "            distance = np.linalg.norm(p_prime - np.array([match[1][0], match[1][1]]))\n",
    "            # If the distance is less than the threshold, increment the number of inliers\n",
    "            if distance < threshold:\n",
    "                inliers += 1\n",
    "                inliers_matches.append(match)\n",
    "        # If the current homography matrix has more inliers than the best one, update the best one\n",
    "        if inliers > best_inliers:\n",
    "            best_inliers = inliers\n",
    "            best_inliers_matches = inliers_matches\n",
    "            best_homography = H\n",
    "        if iter % 100 == 0:\n",
    "            print(f'iteration {iter}, best inliers: {best_inliers}')\n",
    "    return best_homography, best_inliers_matches\n",
    "\n",
    "\n",
    "        \n",
    "# function for calculating the homography matrix without using cv2.findHomography\n",
    "def calculate_homography(matches):\n",
    "    src_pts = np.array([[match[0][0], match[0][1]] for match in matches])\n",
    "    dst_pts = np.array([[match[1][0], match[1][1]] for match in matches])\n",
    "    A = []\n",
    "    for i in range(len(matches)):\n",
    "        x, y = src_pts[i]\n",
    "        x_prime, y_prime = dst_pts[i]\n",
    "        A.append([x, y, 1, 0, 0, 0, -x_prime * x, -x_prime * y, -x_prime])\n",
    "        A.append([0, 0, 0, x, y, 1, -y_prime * x, -y_prime * y, -y_prime])\n",
    "    A = np.array(A)\n",
    "    U, S, V = np.linalg.svd(A)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    return H\n",
    "\n",
    "\n",
    "# create a function that returns the euclidean distance between two descriptors\n",
    "def euclidean_distance(descriptor1, descriptor2):\n",
    "    return np.linalg.norm(descriptor1 - descriptor2)\n",
    "\n",
    "\n",
    "def get_best_match(descriptor, descriptors):\n",
    "    # Calculate the Euclidean distances\n",
    "    distances = np.linalg.norm(descriptors - descriptor, axis=(1))\n",
    "\n",
    "    # Find the index of the best match\n",
    "    best_match_idx = np.argmin(distances)\n",
    "    min_distance = distances[best_match_idx]\n",
    "    # Remove the best match from the distances to find the second best match\n",
    "    distances[best_match_idx] = np.inf\n",
    "    second_best_match_idx = np.argmin(distances)\n",
    "    return best_match_idx, second_best_match_idx, min_distance, distances[second_best_match_idx]\n",
    "\n",
    "\n",
    "def backward_warping_window(x, y, theta, img, window_size):\n",
    "    output_img = np.zeros((window_size, window_size))\n",
    "    for i in range(window_size):\n",
    "        for j in range(window_size):\n",
    "            # Translate the coordinates to be relative to the center of the window\n",
    "            x_rel, y_rel = i - window_size // 2, j - window_size // 2\n",
    "            # Apply rotation transformation\n",
    "            x_rot = int(x_rel * math.cos(theta) - y_rel * math.sin(theta)) + x\n",
    "            y_rot = int(x_rel * math.sin(theta) + y_rel * math.cos(theta)) + y\n",
    "            \n",
    "            # Ensure the rotated coordinates are within the bounds of the original image\n",
    "            if 0 <= x_rot < img.shape[0] and 0 <= y_rot < img.shape[1]:\n",
    "                # Perform bilinear interpolation\n",
    "                x0, y0 = int(x_rot), int(y_rot)\n",
    "                x1, y1 = min(x0 + 1, img.shape[0]-1) , min(y0 + 1, img.shape[1]-1)\n",
    "                dx, dy = x_rot - x0, y_rot - y0\n",
    "                # try:\n",
    "                output_img[i, j] = (1 - dx) * (1 - dy) * img[x0, y0] + dx * (1 - dy) * img[x0, y1] + (1 - dx) * dy * img[x1, y0] + dx * dy * img[x1, y1]\n",
    "                # except IndexError:\n",
    "                #     continue\n",
    "                \n",
    "    return output_img\n",
    "\n",
    "# normalize the corners windows (I = I - mean(I)/ std(I))\n",
    "def normalize_window(window):\n",
    "    normalized_window = window - np.mean(window)\n",
    "    if np.std(window) != 0:\n",
    "        normalized_window = normalized_window / np.std(window)\n",
    "    return normalized_window\n",
    "\n",
    "\n",
    "def get_mops_descriptor(x, y, theta, img, window_size):\n",
    "    # Perform backward warping\n",
    "    warped_window = backward_warping_window(x, y, theta, img, window_size)\n",
    "    # Normalize the window\n",
    "    descriptor = normalize_window(warped_window)\n",
    "    return descriptor\n",
    "\n",
    "\n",
    "def get_mops_descriptors(pyramid, corners_arr, descriptor_window_size, scale_added_to_get_descriptor):\n",
    "    descriptors = []\n",
    "    for scale in range(scales):\n",
    "        img_in_scale = pyramid[scale]['G']\n",
    "        img_in_2_scales_above = pyramid[scale+scale_added_to_get_descriptor]['G']\n",
    "        corners = corners_arr[scale]\n",
    "        # calculate the gradient of the image at the current scale\n",
    "        Ix = convolve(img_in_scale, np.array([[-1, 1]]))\n",
    "        Iy = convolve(img_in_scale, np.array([[-1], [1]]))\n",
    "        # Ix = cv2.Sobel(img_in_scale, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        # Iy = cv2.Sobel(img_in_scale, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        # Iterate over the corners\n",
    "        for x, y in zip(*np.where(corners != 0)):\n",
    "            # extract the angle of the corner by the gradient of the image\n",
    "            theta = math.atan2(Iy[x, y], Ix[x, y])\n",
    "            x_in_higher_scale, y_in_higher_scale = get_coordinated_in_higher_scale(x, y, scale_added_to_get_descriptor)\n",
    "            # Perform backward warping on the scale+2\n",
    "            descriptor = get_mops_descriptor(x_in_higher_scale, y_in_higher_scale, theta, img_in_2_scales_above, descriptor_window_size)\n",
    "            # Compute the difference between the original and warped windows\n",
    "            descriptors.append([x, y, scale, theta, descriptor]) \n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def get_coordinated_in_higher_scale(x, y, scale_added):\n",
    "    return x // (2 ** scale_added), y // (2 ** scale_added)\n",
    "\n",
    "\n",
    "# implement the function that returns the transformed image using the homography matrix\n",
    "def transform_image(img, H):\n",
    "    # Create the inverse transformation matrix\n",
    "    H_inv = np.linalg.inv(H)\n",
    "    # Create the output image\n",
    "    output_img = np.zeros_like(img)\n",
    "    # Iterate over the pixels in the output image\n",
    "    for i in range(output_img.shape[0]):\n",
    "        for j in range(output_img.shape[1]):\n",
    "            # Apply the inverse transformation to the pixel coordinates\n",
    "            p = np.array([i, j, 1])\n",
    "            p_prime = np.dot(H_inv, p)\n",
    "            p_prime = (p_prime / p_prime[2])[:2]\n",
    "            # Check if the transformed pixel is within the bounds of the input image\n",
    "            if 0 <= p_prime[0] < img.shape[0] and 0 <= p_prime[1] < img.shape[1]:\n",
    "                # Perform bilinear interpolation\n",
    "                x0, y0 = int(p_prime[0]), int(p_prime[1])\n",
    "                x1, y1 = min(x0 + 1, img.shape[0]-1) , min(y0 + 1, img.shape[1]-1)\n",
    "                dx, dy = p_prime[0] - x0, p_prime[1] - y0\n",
    "                # Perform the interpolation\n",
    "                output_img[i, j] = (1 - dx) * (1 - dy) * img[x0, y0] + dx * (1 - dy) * img[x0, y1] + (1 - dx) * dy * img[x1, y0] + dx * dy * img[x1, y1]\n",
    "    return output_img\n",
    "\n",
    "\n",
    "def remove_corners_from_high_res_images_edges(R_arr_high, high_pyramid):\n",
    "    for i in range(len(R_arr_high)):\n",
    "        print('before', np.sum(R_arr_high[i] != 0))\n",
    "        img = deepcopy(R_arr_high[i])\n",
    "        R_arr_high[i] = remove_corners_from_high_res_image_edges(high_pyramid[i]['G'], img)\n",
    "        print('after', np.sum(R_arr_high[i] != 0))\n",
    "        \n",
    "        \n",
    "def create_mask_of_background(gray):\n",
    "    # Threshold the grayscale image to create a binary mask\n",
    "    _, mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Dilate the mask slightly to include nearby pixels\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Invert the mask (to remove the background)\n",
    "    mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# create an algorithm that gets an image and its corners (an image with the same size with the R values) and does the following:\n",
    "def remove_corners_from_high_res_image_edges(img, corners):\n",
    "    # create a mask that indicates if in the pixels 5x5 window there are more than 5 0 values\n",
    "    mask = create_mask_of_background(img)    \n",
    "    new_corners = deepcopy(corners)\n",
    "    new_corners[mask == 255] = 0\n",
    "    return new_corners\n",
    "\n",
    "\n",
    "def get_corners_arr(R_arr, threshold, max_corners):\n",
    "    corners_arr = []\n",
    "    for R in R_arr:\n",
    "        R[R < threshold] = 0\n",
    "        if np.sum(R != 0) > max_corners:\n",
    "            R[R < np.sort(R[R != 0])[-max_corners]] = 0\n",
    "        corners_arr.append(R)\n",
    "    return corners_arr\n",
    "\n",
    "\n",
    "\n",
    "def wavelet_transform(image, wavelet='haar'):\n",
    "    # Perform 2D Discrete Wavelet Transform\n",
    "    coeffs = pywt.dwt2(image, wavelet)\n",
    "    return flatten_coeffs(coeffs)\n",
    "\n",
    "# create a function that take coeffs and returns 1D array of the coefficients\n",
    "def flatten_coeffs(coeffs):\n",
    "    cA, (cH, cV, cD) = coeffs\n",
    "    return np.concatenate([cA.flatten(), cH.flatten(), cV.flatten(), cD.flatten()])\n",
    "\n",
    "# create a function that returns the best matches for each descriptor\n",
    "def get_all_matches(descriptors1, descriptors2):\n",
    "    matches = []\n",
    "    descriptors1_wavelets = np.array([wavelet_transform(d[4]) for d in descriptors1])\n",
    "    descriptors2_wavelets = np.array([wavelet_transform(d[4]) for d in descriptors2])\n",
    "    for i, wavelet in enumerate(descriptors1_wavelets):\n",
    "        best_match_idx, second_best_match_idx, NN_1, NN_2  = get_best_match(wavelet, descriptors2_wavelets)\n",
    "        best_match = descriptors2[best_match_idx]\n",
    "        # second_best_match = descriptors2[second_best_match_idx]\n",
    "        # if best_match is not None and second_best_match is not None:\n",
    "            # if not NN_1 or not NN_2 or NN_2 == 0:\n",
    "            #     continue\n",
    "        matches.append([descriptors1[i], best_match, NN_1 / NN_2])\n",
    "        if i % 500 == 0:\n",
    "            print(f'iteration {i}') \n",
    "    matches.sort(key=lambda x: x[2])\n",
    "    return matches\n",
    "\n",
    "def get_best_matches(matches, threshold):\n",
    "    best_matches = []\n",
    "    for match in matches:\n",
    "        if match[2] < threshold:\n",
    "            best_matches.append(match)\n",
    "    return best_matches\n",
    "\n",
    "\n",
    "# example usage of the munually implemented functions for image blending\n",
    "\n",
    "# img_low_res_path = './assets/desert_low_res.jpg'\n",
    "# img_high_res_path = './assets/desert_high_res.png'    \n",
    "# img_low_res = open_image(img_low_res_path)\n",
    "# img_high_res = open_image(img_high_res_path)\n",
    "# img_low_res_gray = get_gray_img_mat(img_low_res)\n",
    "# img_high_res_gray = get_gray_img_mat(img_high_res)\n",
    "# low_pyramid = create_pyramid(img_low_res_gray, harris_window_size)\n",
    "# high_pyramid = create_pyramid(img_high_res_gray, harris_window_size)\n",
    "# R_arr_low = multiscale_harris_corner_detector(low_pyramid, scales, harris_window_size, k, suppression_window_size)\n",
    "# R_arr_high = multiscale_harris_corner_detector(high_pyramid, scales, harris_window_size, k, suppression_window_size)\n",
    "# corners_arr_low = get_corners_arr(R_arr_low, threshold, max_corners_per_scale)\n",
    "# corners_arr_high = get_corners_arr(R_arr_high, threshold, max_corners_per_scale)\n",
    "# descriptors_low = get_mops_descriptors(low_pyramid, corners_arr_low, descriptor_window_size, scale_added_to_get_descriptor)\n",
    "# descriptors_high = get_mops_descriptors(high_pyramid, corners_arr_high, descriptor_window_size, scale_added_to_get_descriptor)\n",
    "# all_matches = get_all_matches(descriptors_high, descriptors_low)\n",
    "# matches = get_best_matches(all_matches, matches_ratio_threshold)\n",
    "# matches_with_original_coordinates = get_matches_with_original_coordinates(matches)\n",
    "# H, inliers = ransac(matches_with_original_coordinates, 1000, 10)\n",
    "# transformed_img = transform_image(img_high_res, H)\n",
    "# mask = np.any(transformed_img != 0, axis=-1).astype(int)\n",
    "# blended_img = deepcopy(img_low_res)\n",
    "# blended_img[mask == 1] = transformed_img[mask == 1]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T17:03:49.607281Z",
     "start_time": "2024-02-28T17:03:49.603598Z"
    }
   },
   "id": "45132be316144115",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.838200Z",
     "start_time": "2024-02-28T15:06:27.829546Z"
    }
   },
   "id": "44e0304c8288a8b5",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.838616Z",
     "start_time": "2024-02-28T15:06:27.831607Z"
    }
   },
   "id": "4ffc39bac7f0a927",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.838906Z",
     "start_time": "2024-02-28T15:06:27.833530Z"
    }
   },
   "id": "b880cea964a86f38",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.839189Z",
     "start_time": "2024-02-28T15:06:27.835880Z"
    }
   },
   "id": "5f25b23acaa81af3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.840688Z",
     "start_time": "2024-02-28T15:06:27.837945Z"
    }
   },
   "id": "b696798f12fa73a5",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.841530Z",
     "start_time": "2024-02-28T15:06:27.840355Z"
    }
   },
   "id": "a289cb6b52857050",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.849755Z",
     "start_time": "2024-02-28T15:06:27.841612Z"
    }
   },
   "id": "99f1d1635d5642cb",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.850231Z",
     "start_time": "2024-02-28T15:06:27.843583Z"
    }
   },
   "id": "6b3ba2881be7921b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.850577Z",
     "start_time": "2024-02-28T15:06:27.845536Z"
    }
   },
   "id": "3f2516dbad1c49db",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.868356Z",
     "start_time": "2024-02-28T15:06:27.849393Z"
    }
   },
   "id": "60d53f63179789e1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.868674Z",
     "start_time": "2024-02-28T15:06:27.851384Z"
    }
   },
   "id": "5def9aa2e6d629ae",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:27.868933Z",
     "start_time": "2024-02-28T15:06:27.853278Z"
    }
   },
   "id": "863d8cec15ab1a61",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
